{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro import poutine\n",
    "from pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO, Predictive, NUTS, MCMC\n",
    "from pyro.infer.autoguide import AutoDelta, AutoNormal\n",
    "from pyro.ops.indexing import Vindex\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various copy number sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['hidden_dim'] = 2\n",
    "args['n'] = 20\n",
    "args['x_states'] = np.array([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_x = [0.99, 0.01]\n",
    "\n",
    "len_seq = 5000 \n",
    "x_seq = []\n",
    "y_seq = []\n",
    "x_states = args['x_states'] \n",
    "x_state = 1\n",
    "for i in range(len_seq):\n",
    "    draw = rng.choice(np.arange(args['hidden_dim']), p=probs_x)\n",
    "    if draw == 0:\n",
    "        x_state = x_states[x_states == x_state]\n",
    "    elif draw == 1:\n",
    "        x_state = x_states[x_states != x_state]\n",
    "        \n",
    "    x_seq.append(x_state)\n",
    "    y_seq.append(rng.binomial(args['n'], x_state/(x_state+1)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,2))\n",
    "plt.plot(np.array(x_seq)/(np.array(x_seq) + 1) * args['n'], color='red')\n",
    "plt.scatter(np.arange(len_seq), np.array(y_seq), s=2)\n",
    "#plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_domain_matmul(log_A, log_B):\n",
    "    \"\"\"\n",
    "    log_A : m x n\n",
    "    log_B : n x p\n",
    "    output : m x p matrix\n",
    "\n",
    "    Normally, a matrix multiplication\n",
    "    computes out_{i,j} = sum_k A_{i,k} x B_{k,j}\n",
    "\n",
    "    A log domain matrix multiplication\n",
    "    computes out_{i,j} = logsumexp_k log_A_{i,k} + log_B_{k,j}\n",
    "    \"\"\"\n",
    "    m = log_A.shape[0]\n",
    "    n = log_A.shape[1]\n",
    "    p = log_B.shape[1]\n",
    "\n",
    "    log_A_expanded = torch.reshape(log_A, (m,n,1))\n",
    "    log_B_expanded = torch.reshape(log_B, (1,n,p))\n",
    "\n",
    "    elementwise_sum = log_A_expanded + log_B_expanded\n",
    "    out = torch.logsumexp(elementwise_sum, dim=1)\n",
    "    return out\n",
    "\n",
    "def maxmul(log_A, log_B):\n",
    "    \"\"\"\n",
    "    log_A : m x n\n",
    "    log_B : n x p\n",
    "    output : m x p matrix; maxval, argmaxval\n",
    "\n",
    "    Similar to the log domain matrix multiplication,\n",
    "    this computes out_{i,j} = max_k log_A_{i,k} + log_B_{k,j}\n",
    "    \"\"\"\n",
    "    m = log_A.shape[0]\n",
    "    n = log_A.shape[1]\n",
    "    p = log_B.shape[1]\n",
    "\n",
    "    log_A_expanded = torch.stack([log_A] * p, dim=2)\n",
    "    log_B_expanded = torch.stack([log_B] * m, dim=0)\n",
    "\n",
    "    elementwise_sum = log_A_expanded + log_B_expanded\n",
    "    out1,out2 = torch.max(elementwise_sum, dim=1)\n",
    "\n",
    "    return out1,out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch, state, \n",
    "\n",
    "class TransitionModel(torch.nn.Module):\n",
    "    def __init__(self, N_states):\n",
    "        super(TransitionModel, self).__init__()\n",
    "        self.N_states = N_states\n",
    "        self.unnormalized_transition_matrix = torch.nn.Parameter(torch.randn(N_states,N_states))\n",
    "    def forward(self, log_alpha):\n",
    "        log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
    "\n",
    "        # Matrix multiplication in the log domain\n",
    "        out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
    "        return out\n",
    "    def maxmul(self, log_alpha):\n",
    "        log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
    "\n",
    "        out1, out2 = maxmul(log_transition_matrix, log_alpha.transpose(0,1))\n",
    "        return out1.transpose(0,1), out2.transpose(0,1)\n",
    "\n",
    "class EmissionModel(torch.nn.Module):\n",
    "    def __init__(self, N_states, x_states, n):\n",
    "        super(EmissionModel, self).__init__()\n",
    "        self.N_states = N_states\n",
    "        self.x_states = x_states\n",
    "        self.n = n\n",
    "    def forward(self, x_t):\n",
    "        '''\n",
    "        out: batch x state\n",
    "        '''\n",
    "        batch_size = x_t.shape[0]\n",
    "        xs = torch.tensor(self.x_states.reshape((1, self.N_states)))\n",
    "        logprob = torch.distributions.Binomial(self.n, xs / (1 + xs)).log_prob(x_t.reshape((batch_size, 1)))\n",
    "        return logprob\n",
    "        #in log domain\n",
    "\n",
    "class HMM(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(HMM, self).__init__()\n",
    "        self.N_states = args['hidden_dim']\n",
    "        self.x_states = args['x_states']\n",
    "        self.n = args['n']\n",
    "\n",
    "        # A\n",
    "        self.transition_model = TransitionModel(self.N_states)\n",
    "\n",
    "        # b(x_t)\n",
    "        self.emission_model = EmissionModel(self.N_states, self.x_states, self.n)\n",
    "\n",
    "        # pi\n",
    "        self.unnormalized_state_priors = torch.nn.Parameter(torch.randn(self.N_states))\n",
    "\n",
    "        # use the GPU\n",
    "        self.is_cuda = torch.cuda.is_available()\n",
    "        if self.is_cuda: self.cuda()\n",
    "            \n",
    "    def sample(self, T=10):\n",
    "        state_priors = torch.nn.functional.softmax(self.unnormalized_state_priors, dim=0)\n",
    "        transition_matrix = torch.nn.functional.softmax(self.transition_model.unnormalized_transition_matrix, dim=0)\n",
    "        #emission_matrix = torch.nn.functional.softmax(self.emission_model.unnormalized_emission_matrix, dim=1)\n",
    "\n",
    "        # sample initial state\n",
    "        z_t = torch.distributions.categorical.Categorical(state_priors).sample().item()\n",
    "        z = []; x = []\n",
    "        z.append(z_t)\n",
    "        for t in range(0,T):\n",
    "            # sample emission\n",
    "            x_t = torch.distributions.Binomial(self.n, self.x_states[z_t] / (1 + self.x_states[z_t])).sample().item()\n",
    "            x.append(x_t)\n",
    "\n",
    "            # sample transition\n",
    "            z_t = torch.distributions.categorical.Categorical(transition_matrix[:,z_t]).sample().item()\n",
    "            if t < T-1: z.append(z_t)\n",
    "\n",
    "        return x, z\n",
    "    \n",
    "    def forward(self, x, T):\n",
    "        \"\"\"\n",
    "        x : IntTensor of shape (batch size, T_max)\n",
    "        T : IntTensor of shape (batch size)\n",
    "\n",
    "        Compute log p(x) for each example in the batch.\n",
    "        T = length of each example\n",
    "        \"\"\"\n",
    "        if self.is_cuda:\n",
    "            x = x.cuda()\n",
    "            T = T.cuda()\n",
    "\n",
    "        batch_size = x.shape[0]; T_max = x.shape[1]\n",
    "        log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "        log_alpha = torch.zeros(batch_size, T_max, self.N_states)\n",
    "        if self.is_cuda: log_alpha = log_alpha.cuda()\n",
    "\n",
    "        log_alpha[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors\n",
    "        for t in range(1, T_max):\n",
    "            log_alpha[:, t, :] = self.emission_model(x[:,t]) + self.transition_model(log_alpha[:, t-1, :])\n",
    "\n",
    "        # Select the sum for the final timestep (each x may have different length).\n",
    "        log_sums = log_alpha.logsumexp(dim=2)\n",
    "        log_probs = torch.gather(log_sums, 1, T.view(-1,1) - 1)\n",
    "        return log_probs\n",
    "    \n",
    "    def viterbi(self, x, T):\n",
    "        \"\"\"\n",
    "        x : IntTensor of shape (batch size, T_max)\n",
    "        T : IntTensor of shape (batch size)\n",
    "        Find argmax_z log p(x|z) for each (x) in the batch.\n",
    "        \"\"\"\n",
    "        if self.is_cuda:\n",
    "            x = x.cuda()\n",
    "            T = T.cuda()\n",
    "\n",
    "        batch_size = x.shape[0]; T_max = x.shape[1]\n",
    "        log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "        log_delta = torch.zeros(batch_size, T_max, self.N_states).float()\n",
    "        psi = torch.zeros(batch_size, T_max, self.N_states).long()\n",
    "        if self.is_cuda:\n",
    "            log_delta = log_delta.cuda()\n",
    "            psi = psi.cuda()\n",
    "\n",
    "        log_delta[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors\n",
    "        for t in range(1, T_max):\n",
    "            max_val, argmax_val = self.transition_model.maxmul(log_delta[:, t-1, :])\n",
    "            log_delta[:, t, :] = self.emission_model(x[:,t]) + max_val\n",
    "            psi[:, t, :] = argmax_val\n",
    "\n",
    "        # Get the log probability of the best path\n",
    "        log_max = log_delta.max(dim=2)[0]\n",
    "        best_path_scores = torch.gather(log_max, 1, T.view(-1,1) - 1)\n",
    "\n",
    "        # This next part is a bit tricky to parallelize across the batch,\n",
    "        # so we will do it separately for each example.\n",
    "        z_star = []\n",
    "        for i in range(0, batch_size):\n",
    "            z_star_i = [ log_delta[i, T[i] - 1, :].max(dim=0)[1].item() ]\n",
    "        for t in range(T[i] - 1, 0, -1):\n",
    "            z_t = psi[i, t, z_star_i[0]].item()\n",
    "            z_star_i.insert(0, z_t)\n",
    "\n",
    "        z_star.append(z_star_i)\n",
    "        return z_star, best_path_scores # return both the best path and its log probability\n",
    "    \n",
    "    def EM(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "    def amaeba_optimisation_interface(self, transition_matrix, x, T):\n",
    "        mat = transition_matrix.reshape((2,2)).astype('float32')\n",
    "        self.transition_model.unnormalized_transition_matrix = torch.nn.Parameter(torch.tensor(mat))\n",
    "        loss = self.forward(x, T).cpu().data.numpy()\n",
    "        print(loss[0])\n",
    "        return -loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "model = HMM(args)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "dataset = (torch.tensor(y_seq).T, torch.tensor([len(y_seq)]))\n",
    "\n",
    "output = minimize(model.amaeba_optimisation_interface,\n",
    "         np.array([0.12,-0.51,0.2,-0.2]),\n",
    "         args=tuple(dataset),\n",
    "         method='Nelder-Mead',\n",
    "         options={'maxiter': 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.softmax(torch.tensor(output.x.reshape((2,2))), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, lr):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr, weight_decay=0.00001)\n",
    "\n",
    "    def train(self, dataset, n_steps = 1000):\n",
    "        train_loss = []\n",
    "        num_samples = 0\n",
    "        self.model.train()\n",
    "        #print_interval = 50\n",
    "        x,T = dataset\n",
    "        for i in tqdm(range(n_steps)):\n",
    "            #batch_size = len(x)\n",
    "            #num_samples += batch_size\n",
    "            log_probs = self.model(x,T)\n",
    "            loss = -log_probs.mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss.append(loss.cpu().data.numpy().item())\n",
    "            #if idx % print_interval == 0:\n",
    "            #    print(\"loss:\", loss.item())\n",
    "            #    for _ in range(5):\n",
    "            #        sampled_x, sampled_z = self.model.sample()\n",
    "            #        print(decode(sampled_x))\n",
    "            #        print(sampled_z)\n",
    "        return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HMM(args)\n",
    "dataset = (torch.tensor(y_seq).T, torch.tensor([len(y_seq)]))\n",
    "\n",
    "trainer = Trainer(model, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = trainer.train(dataset, n_steps=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_states, logprob = model.viterbi(*dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(random_states)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,2))\n",
    "plt.scatter(np.arange(len_seq), np.array(pred + 1)/(np.array(pred + 1) + 1) * args['n'], color='orange')\n",
    "plt.plot(np.array(x_seq)/(np.array(x_seq) + 1) * args['n'], color='red')\n",
    "plt.scatter(np.arange(len_seq), np.array(y_seq), s=2)\n",
    "#plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.softmax(model.transition_model.unnormalized_transition_matrix, dim=0).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyro implementation\n",
    "(reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,2))\n",
    "#plt.plot(np.array(x_seq)/(np.array(x_seq) + 1) * args['n'], color='red')\n",
    "plt.scatter(np.arange(len_seq), np.array(pred + 1)/(np.array(pred + 1) + 1) * args['n'], color='orange')\n",
    "plt.scatter(np.arange(len_seq), np.array(y_seq), s=2)\n",
    "#plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([inf_trace[f'x_{i}']['value'].numpy() for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_trace = poutine.trace(guide).get_trace(data, length, args)  # record the globals\n",
    "trained_model = poutine.replay(model, trace=guide_trace)  # replay the globals\n",
    "\n",
    "def classifier(data, temperature=0):\n",
    "    inferred_model = infer_discrete(trained_model, temperature=temperature,\n",
    "                                    first_available_dim=-2)  # avoid conflict with data plate\n",
    "    trace = poutine.trace(inferred_model).get_trace(data, length, args)\n",
    "    return trace.nodes\n",
    "\n",
    "inf_trace = classifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate, infer_discrete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = Predictive(model, guide=guide, num_samples=20)(data, length, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace['probs_x'].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(np.array(y_seq).astype('float32')).T\n",
    "length = data.shape[-1]\n",
    "\n",
    "n_steps = 1000\n",
    "loss_list = []\n",
    "for step in tqdm(range(n_steps)):\n",
    "    loss = svi.step(data, length, args=args)\n",
    "    loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(42)\n",
    "pyro.clear_param_store()\n",
    "guide = AutoNormal(\n",
    "    poutine.block(model, expose_fn=lambda msg: msg[\"name\"].startswith(\"probs_\"))\n",
    ")\n",
    "optim = Adam({\"lr\": 0.005})\n",
    "elbo = TraceEnum_ELBO(\n",
    "            max_plate_nesting=1,\n",
    "            )\n",
    "\n",
    "svi = SVI(model, guide, optim, elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(sequence, length, args, batch_size=None, include_prior=True):\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_x = pyro.sample(\n",
    "            \"probs_x\",\n",
    "            dist.Dirichlet(0.9 * torch.eye(args['hidden_dim']) + 0.1).to_event(1),\n",
    "        )\n",
    "          \n",
    "    # In this first model we'll sequentially iterate over sequences in a\n",
    "    # minibatch; this will make it easy to reason about tensor shapes.\n",
    "    majAll_plate = pyro.plate(\"majAll\", length, dim=-1)\n",
    "    \n",
    "    x = 0\n",
    "    for t in pyro.markov(range(length)):\n",
    "        # On the next line, we'll overwrite the value of x with an updated\n",
    "        # value. If we wanted to record all x values, we could instead\n",
    "        # write x[t] = pyro.sample(...x[t-1]...).\n",
    "        x = pyro.sample(\n",
    "            \"x_{}\".format(t),\n",
    "            dist.Categorical(probs_x[x]),\n",
    "            infer={\"enumerate\": \"parallel\"},\n",
    "        )\n",
    "        with majAll_plate:\n",
    "            p = Vindex(torch.tensor(args['x_states']))[x]\n",
    "            y = pyro.sample(\n",
    "                    \"y_{}\".format(t),\n",
    "                    dist.Binomial(torch.tensor(np.array(args['n']).astype('float32')),\n",
    "                                  p/(p + 1)),\n",
    "                    obs=sequence[:,t],\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
